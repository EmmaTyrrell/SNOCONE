{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7766ff-e9ba-4ae5-9aa8-336008b0ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# pseduo code\n",
    "import sys\n",
    "sys.path.append(\"/projects/emty9224/GitCode/SNOCONE/\")\n",
    "from CNN_memoryOptimization import*\n",
    "from CNN_preProcessing import*\n",
    "from CNN_benchmarks import*\n",
    "from CNN_modelArchitectureBlocks import* \n",
    "import rasterio\n",
    "import shap\n",
    "import pandas as pd\n",
    "from rasterio.mask import mask\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from rasterio.windows import from_bounds\n",
    "import psutil\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.losses import Loss\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "start_year = 2022\n",
    "end_year = 2024\n",
    "Domain = \"Rockies\"\n",
    "GPU = \"N\"\n",
    "output_txt = \"Y\"\n",
    "nonFreezeLayers = -8\n",
    "penalty_weight = 1\n",
    "penalties_used = [\"fSCA\", \"LearningRateScaling\"]\n",
    "low_snow_weight=0   \n",
    "swe_threshold=0.01\n",
    "fsca_threshold=0.1\n",
    "low_threshold=0.05\n",
    "penalty_scale= 0\n",
    "learningRateTesting = 1e-3\n",
    "\n",
    "# workspaces\n",
    "WorkspaceBase = f\"/pl/active/hydroServer/ASO_DeepLearningData/CNN_v.02/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "DMFSCAWorkspace = WorkspaceBase + \"Rockies_DMFSCA/\"\n",
    "final_activation = 'relu'\n",
    "architectures = ['Baseline']  # Options are: Baseline, AdvancedBaseline, ResNet18, ResNet34, ResNet50, CustomSWE\n",
    "\n",
    "test_groups = [\n",
    "            ('Group1', 2025, 'G1'),\n",
    "            ('Group2', 2025, 'G2'), \n",
    "            ('Group3', 2025, 'G3'),\n",
    "            ('Group4', 2025, 'G4'),\n",
    "            ('Group5', 2025, 'G5'),\n",
    "            ('Group6', 2025, 'G6')\n",
    "        ]\n",
    "\n",
    "_mae_metric = MeanAbsoluteError()\n",
    "_mse_metric = MeanSquaredError()\n",
    "_rmse_metric = MeanSquaredError()\n",
    "\n",
    "# # Check GPU availability\n",
    "if GPU == \"Y\":\n",
    "    print(\"\\n Checking and start running on GPU\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Configure GPU memory growth (prevents TensorFlow from allocating all GPU memory)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Verify TensorFlow is using GPU\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"GPU computation result:\", c)\n",
    "\n",
    "for architecture in architectures:\n",
    "    print(architecture)\n",
    "    # shapeChecks = \"N\"\n",
    "    \n",
    "    ## seting folder\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "   \n",
    "    # create folder for model outputs\n",
    "    os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "    inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n",
    "\n",
    "    if output_txt == 'Y':\n",
    "        f = open(inter_model_outWorkspace + f\"code_output_{timestamp}.txt\", \"a\")\n",
    "        sys.stdout = f\n",
    "        \n",
    "    print(\"MODEL OUTPUTS TO BE PRINTED TO THIS DOC\")\n",
    "    print(f\"{Domain}\")\n",
    "    print(f\"\\nPenalities: {penalties_used}\")\n",
    "    print(f\"fSCA Penality Weight: {penalty_weight}\")\n",
    "    print(f\"Low Snow Weight: {low_snow_weight}\")\n",
    "    print(f\"Low Snow Threshold: {low_threshold}\")\n",
    "    print(f\"Low Snow Penality Scale: {penalty_scale}\")\n",
    "    print(\"\\n THIS CODE DOES DYNAMIC FREEZING LAYERS AND LEARNING RATES FOR TEST GROUPS\")\n",
    "    \n",
    "    X, y, featureNames = target_feature_stacks(start_year=start_year, \n",
    "                                               end_year=end_year, \n",
    "                                               WorkspaceBase=WorkspaceBase, \n",
    "                                               ext = \"nonull_fnl.tif\", \n",
    "                                               vegetation_path = tree_workspace, \n",
    "                                               landCover_path = land_workspace, \n",
    "                                               phv_path = phv_features , \n",
    "                                               target_shape=(256,256),\n",
    "                                               shapeChecks=\"Y\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Shape of input data\")\n",
    "    print(f\"feature shape: {X.shape}\")\n",
    "    print(f\"target shape: {y.shape}\")\n",
    "    feat_shape = X.shape\n",
    "    featNo = feat_shape[-1]\n",
    "    print(featNo)\n",
    "    print(featureNames)\n",
    "    \n",
    "    # split between training and test data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "    print(\"***\")\n",
    "    print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "    print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "    print(\"***\")\n",
    "    x_trainShape = X_train.shape\n",
    "    x_validShape = X_valid.shape\n",
    "\n",
    "    # Assuming featNo and final_activation are defined in your original code\n",
    "    featNo = featNo  # Replace with your actual feature count\n",
    "    final_activation = final_activation  # Replace with your actual activation\n",
    "    \n",
    "    # Create the model\n",
    "    model = model_implementation(featNo, architecture, final_activation)\n",
    "    \n",
    "    # Your existing custom loss function\n",
    "    from tensorflow.keras.losses import MeanSquaredError\n",
    "    from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "    \n",
    "    custom_loss_fn = make_swe_fsca_loss(\n",
    "        base_loss_fn=MeanSquaredError(),\n",
    "        penalty_weight=penalty_weight,\n",
    "        swe_threshold=swe_threshold,\n",
    "        fsca_threshold=fsca_threshold,\n",
    "        mask_value=-1\n",
    "    )\n",
    "    \n",
    "    # Compile with your existing setup\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=custom_loss_fn,\n",
    "        metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # establish the model\n",
    "    best_weights_path = f\"{inter_model_outWorkspace}/best_weights_{timestamp}.h5\"\n",
    "    checkpoint = ModelCheckpoint(best_weights_path, \n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        mode='min'\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor=\"val_masked_rmse\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    batch_size = 32\n",
    "    train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "    valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[checkpoint, early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # manual backup save of weights\n",
    "    backup_weights_path = f\"{inter_model_outWorkspace}/backup_weights_{timestamp}.h5\"\n",
    "    try:\n",
    "        model.save_weights(backup_weights_path)\n",
    "        print(f\"Backup weights saved to: {backup_weights_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save backup weights: {e}\")\n",
    "    \n",
    "    # Also save model architecture as JSON for SHAP compatibility\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"{inter_model_outWorkspace}/model_architecture_{timestamp}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    print(\"Model architecture saved as JSON\")\n",
    "\n",
    "    ## plotting results \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "    # 1. Masked MSE (used as loss)\n",
    "    axs[0].plot(history.history['loss'], label='Train MSE Loss')\n",
    "    axs[0].plot(history.history['val_loss'], label='Val MSE Loss')\n",
    "    axs[0].set_ylabel('MSE Loss')\n",
    "    axs[0].set_title('Masked MSE')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # 2. Masked RMSE\n",
    "    axs[1].plot(history.history['masked_rmse'], label='Train RMSE')\n",
    "    axs[1].plot(history.history['val_masked_rmse'], label='Val RMSE')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].set_title('Masked RMSE')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    # 3. Masked MAE\n",
    "    axs[2].plot(history.history['masked_mae'], label='Train MAE')\n",
    "    axs[2].plot(history.history['val_masked_mae'], label='Val MAE')\n",
    "    axs[2].set_ylabel('MAE')\n",
    "    axs[2].set_title('Masked MAE')\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(inter_model_outWorkspace + \"Model_error_epochs.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add metrics to recurring error tracking sheet\n",
    "    metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "    best_metrics = {}\n",
    "    print(\"\\nValidation Metric Progression:\")\n",
    "    for metric in metrics_to_track:\n",
    "        values = history.history.get(metric, [])\n",
    "        if values:\n",
    "            best_val = min(values)\n",
    "            best_metrics[metric] = best_val\n",
    "            print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "            print(f\"{metric}: Best = {best_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: Not found in history.\")\n",
    "    print(f\"Final activation function: {final_activation}\")\n",
    "    \n",
    "    # Example variables\n",
    "    feature_csv = modelOuptuts + f\"{Domain}_model_featureList_summary.csv\" \n",
    "    column_name = timestamp  \n",
    "    feature_list = featureNames    \n",
    "    new_column_df = pd.DataFrame({column_name: feature_list})\n",
    "    \n",
    "    # If the file already exists, load it and append the new column\n",
    "    if os.path.exists(feature_csv):\n",
    "        existing_df = pd.read_csv(feature_csv)\n",
    "        # Reindex the existing dataframe \n",
    "        max_len = max(len(existing_df), len(new_column_df))\n",
    "        existing_df = existing_df.reindex(range(max_len))\n",
    "        new_column_df = new_column_df.reindex(range(max_len))\n",
    "        # Combine horizontally\n",
    "        combined_df = pd.concat([existing_df, new_column_df], axis=1)\n",
    "    else:\n",
    "        combined_df = new_column_df\n",
    "    \n",
    "    # Save back to CSV\n",
    "    combined_df.to_csv(feature_csv, index=False)\n",
    "    \n",
    "    # add metrics to csv\n",
    "    modelStatsCSV = modelOuptuts + f\"{Domain}_modelSummary_stats.csv\"\n",
    "    \n",
    "    baseline_stats = {\n",
    "        'ModelRun':[timestamp],\n",
    "        'FeatureNum': [featNo],\n",
    "        'Architecture': [architecture],\n",
    "        'FinalActivation': [final_activation],\n",
    "        'X_TrainShape': [x_trainShape[0]],\n",
    "        'X_ValidShape': [x_validShape[0]],\n",
    "        'RMSE': [best_metrics['val_masked_rmse']],\n",
    "        'MSE': [best_metrics['val_masked_mse']], \n",
    "        'MAE': [best_metrics['val_masked_mae']]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(baseline_stats)\n",
    "    \n",
    "    # Append or write new file\n",
    "    if os.path.exists(modelStatsCSV):\n",
    "        df.to_csv(modelStatsCSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(modelStatsCSV, index=False)\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid, X, y\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    del history \n",
    "    clear_memory()\n",
    "\n",
    "    # create folder\n",
    "    print(\"\\n ############### MOVING TO TEST GROUPS OF DATA ###############\")\n",
    "    for group_name, year, group_id in test_groups:\n",
    "        print(f\"\\nTesting {group_name}\")\n",
    "        print(f\"Adjusting the last {nonFreezeLayers} layers\")\n",
    "        print(f\"Learning rate = {learningRateTesting}\")\n",
    "        print(f\"fSCA Penalty rate = {penalty_weight}\")\n",
    "        \n",
    "        # Create output folder\n",
    "        out_folder = inter_model_outWorkspace + f\"outTifs_{group_id}_yPreds_tifs/\"\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        print(\"made output directory\")\n",
    "        \n",
    "        # Load test data\n",
    "        X_train_g, y_train_g, g_train_extents, g_train_crs = target_feature_stacks_testGroups(\n",
    "            year=year,\n",
    "            target_splits_path=WorkspaceBase + f\"test_groups/{group_name}/train/\",\n",
    "            fSCA_path=WorkspaceBase + f\"{year}/fSCA/\",\n",
    "            DMFSCA_path=WorkspaceBase + f\"{year}/DMFSCA/\",\n",
    "            vegetation_path=WorkspaceBase + \"treeCover/\",\n",
    "            landCover_path=land_workspace,\n",
    "            phv_path=WorkspaceBase + \"features/scaled/\",\n",
    "            extension_filter=\".tif\",\n",
    "            desired_shape=(256, 256),\n",
    "            debug_output_folder=\"./debug_outputs/\",\n",
    "            num_of_channels=featNo,\n",
    "            shapeChecks=\"Y\"\n",
    "        )\n",
    "        \n",
    "        X_test_g, y_test_g, g_test_extents, g_test_crs = target_feature_stacks_testGroups(\n",
    "            year=year,\n",
    "            target_splits_path=WorkspaceBase + f\"test_groups/{group_name}/test/\",\n",
    "            fSCA_path=WorkspaceBase + f\"{year}/fSCA/\",\n",
    "            DMFSCA_path=WorkspaceBase + f\"{year}/DMFSCA/\",\n",
    "            vegetation_path=WorkspaceBase + \"treeCover/\",\n",
    "            landCover_path=land_workspace,\n",
    "            phv_path=WorkspaceBase + \"features/scaled/\",\n",
    "            extension_filter=\".tif\",\n",
    "            desired_shape=(256, 256),\n",
    "            debug_output_folder=\"./debug_outputs/\",\n",
    "            num_of_channels=featNo,\n",
    "            shapeChecks=\"Y\"\n",
    "        )\n",
    "        \n",
    "        # Convert to arrays\n",
    "        X_train_array = np.array(X_train_g)\n",
    "        y_train_array = np.array(y_train_g)\n",
    "        X_test_array = np.array(X_test_g)\n",
    "        y_test_array = np.array(y_test_g)\n",
    "        print(f\"X_train shape: {X_train_array.shape}\")\n",
    "        print(f\"y_train shape: {y_train_array.shape}\")\n",
    "        print(f\"X_test shape: {X_test_array.shape}\")\n",
    "        print(f\"y_test shape: {y_test_array.shape}\")\n",
    "\n",
    "        # reload loss functions\n",
    "        custom_loss_fn = make_swe_fsca_loss(\n",
    "            base_loss_fn=MeanSquaredError(),\n",
    "            penalty_weight=penalty_weight,\n",
    "            swe_threshold=swe_threshold,\n",
    "            fsca_threshold=fsca_threshold,\n",
    "            mask_value=-1\n",
    "        )\n",
    "        # add in test model with the weights and test different weights options\n",
    "        operational_model = model_implementation(featNo, architecture, final_activation)\n",
    "\n",
    "        # Try multiple weight file locations\n",
    "        weight_files = [\n",
    "            f\"{inter_model_outWorkspace}/best_weights_{timestamp}.h5\",\n",
    "            f\"{inter_model_outWorkspace}/backup_weights_{timestamp}.h5\",\n",
    "            f\"{inter_model_outWorkspace}/best_weights_{timestamp}.weights.h5\"\n",
    "        ]\n",
    "        \n",
    "        weights_loaded = False\n",
    "        for weight_file in weight_files:\n",
    "            if os.path.exists(weight_file):\n",
    "                try:\n",
    "                    print(f\"Attempting to load weights from: {weight_file}\")\n",
    "                    operational_model.load_weights(weight_file)\n",
    "                    print(f\"Successfully loaded weights from: {weight_file}\")\n",
    "                    weights_loaded = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load weights from {weight_file}: {e}\")\n",
    "\n",
    "        if not weights_loaded:\n",
    "            print(\"ERROR: Could not load any weights file. Check if training completed successfully.\")\n",
    "            print(\"Available files in directory:\")\n",
    "            for file in os.listdir(inter_model_outWorkspace):\n",
    "                if 'weights' in file or file.endswith('.h5'):\n",
    "                    print(f\"  - {file}\")\n",
    "            continue  # Skip this test group\n",
    "\n",
    "        # Test different freezing levels and train with best one\n",
    "        print(f\"\\n Testing and training with optimal freezing for {group_name}...\")\n",
    "        test_levels = [-4, -6, -8, -10, -12]\n",
    "        print(f\"Testing levels: {test_levels}\")\n",
    "        \n",
    "        freezing_results = {}\n",
    "        best_freezing_level = -8  \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for freeze_level in test_levels:\n",
    "            print(f\"\\n--- Testing {freeze_level} unfrozen layers ---\")\n",
    "            \n",
    "            # Reset all layers to trainable first\n",
    "            for layer in operational_model.layers:\n",
    "                layer.trainable = True\n",
    "            \n",
    "            # Apply current freezing level\n",
    "            for layer in operational_model.layers[:freeze_level]:\n",
    "                layer.trainable = False\n",
    "            \n",
    "            frozen_count = len([l for l in operational_model.layers if not l.trainable])\n",
    "            trainable_count = len([l for l in operational_model.layers if l.trainable])\n",
    "            \n",
    "            print(f\"Frozen: {frozen_count}, Trainable: {trainable_count}\")\n",
    "            \n",
    "            # Learning rate logger for this test\n",
    "            class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "                    self.lrs = []\n",
    "                    \n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    lr = self.model.optimizer.learning_rate.numpy()\n",
    "                    self.lrs.append(lr)\n",
    "            \n",
    "            lr_logger = LearningRateLogger()\n",
    "            \n",
    "            # Compile with this configuration\n",
    "            operational_model.compile(\n",
    "                optimizer=Adam(learning_rate=learningRateTesting),\n",
    "                loss=custom_loss_fn,\n",
    "                metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "            )\n",
    "            \n",
    "            # Setup callbacks for this test\n",
    "            test_early_stop = EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=5,  # Shorter patience for testing\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            test_reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=2,  # Shorter patience for testing\n",
    "                min_lr=1e-6,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Train with full fine-tuning setup (not just 5 epochs)\n",
    "            test_history = operational_model.fit(\n",
    "                X_train_array, y_train_array,\n",
    "                batch_size=16,\n",
    "                epochs=15,  # Reasonable epochs for testing\n",
    "                validation_split=0.2,\n",
    "                callbacks=[test_early_stop, test_reduce_lr, lr_logger],\n",
    "                verbose=0  # Silent for testing\n",
    "            )\n",
    "            \n",
    "            # Get final validation loss\n",
    "            final_val_loss = test_history.history['val_loss'][-1]\n",
    "            final_val_rmse = test_history.history['val_masked_rmse'][-1]\n",
    "            best_val_loss_in_test = min(test_history.history['val_loss'])\n",
    "            \n",
    "            # Check for improvement trend\n",
    "            val_losses = test_history.history['val_loss']\n",
    "            improvement = val_losses[0] - val_losses[-1]\n",
    "            \n",
    "            # Learning rate info\n",
    "            final_lr = lr_logger.lrs[-1]\n",
    "            lr_reductions = sum(1 for i in range(1, len(lr_logger.lrs)) if lr_logger.lrs[i] < lr_logger.lrs[i-1])\n",
    "            \n",
    "            freezing_results[freeze_level] = {\n",
    "                'final_val_loss': final_val_loss,\n",
    "                'best_val_loss': best_val_loss_in_test,\n",
    "                'final_val_rmse': final_val_rmse,\n",
    "                'improvement': improvement,\n",
    "                'frozen_count': frozen_count,\n",
    "                'trainable_count': trainable_count,\n",
    "                'final_lr': final_lr,\n",
    "                'lr_reductions': lr_reductions,\n",
    "                'epochs_trained': len(test_history.history['loss']),\n",
    "                'lr_progression': lr_logger.lrs\n",
    "            }\n",
    "            \n",
    "            print(f\"Val Loss: {final_val_loss:.4f}, Best: {best_val_loss_in_test:.4f}, Final LR: {final_lr:.2e}, LR Reductions: {lr_reductions}\")\n",
    "            \n",
    "            # Track the overall best\n",
    "            if best_val_loss_in_test < best_val_loss:\n",
    "                best_val_loss = best_val_loss_in_test\n",
    "                best_freezing_level = freeze_level\n",
    "        \n",
    "        print(f\"\\n Best freezing level for {group_name}: {best_freezing_level} (Best Val Loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "        # Apply the best freezing level for final results\n",
    "        for layer in operational_model.layers[:best_freezing_level]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        frozen_count = len([l for l in operational_model.layers if not l.trainable])\n",
    "        trainable_count = len([l for l in operational_model.layers if l.trainable])\n",
    "        \n",
    "        print(f\"Applied optimal freezing: {frozen_count} frozen, {trainable_count} trainable layers\")\n",
    "        \n",
    "        # Get the best results for reporting\n",
    "        best_results = freezing_results[best_freezing_level]\n",
    "        final_lr = best_results['final_lr']\n",
    "        lr_reductions = best_results['lr_reductions']\n",
    "        adaptation_epochs = best_results['epochs_trained']\n",
    "        \n",
    "        print(f\"Best configuration achieved:\")\n",
    "        print(f\"  Final LR: {final_lr:.2e}\")\n",
    "        print(f\"  LR reductions: {lr_reductions}\")\n",
    "        print(f\"  Epochs trained: {adaptation_epochs}\")\n",
    "        \n",
    "\n",
    "        # After fine-tuning, analyze the adaptation process\n",
    "        print(f\"\\n{group_name} Adaptation Analysis:\")\n",
    "        print(f\"Best val_loss achieved: {best_results['best_val_loss']:.4f}\")\n",
    "        print(f\"Final val_loss: {best_results['final_val_loss']:.4f}\")\n",
    "        print(f\"Total epochs trained: {adaptation_epochs}\")\n",
    "        \n",
    "        # Calculate overfitting gap from best results\n",
    "        overfitting_gap = best_results['final_val_loss'] - best_results['best_val_loss']\n",
    "        print(f\"Overfitting gap: {overfitting_gap:.4f}\")\n",
    "        if overfitting_gap > 0.1:\n",
    "            print(\"Potential overfitting detected!\")\n",
    "        \n",
    "        # Enhanced learning rate analysis\n",
    "        print(f\"\\nLearning Rate Summary:\")\n",
    "        print(f\"Initial LR: {learningRateTesting:.2e}\")\n",
    "        print(f\"Final LR: {final_lr:.2e}\")\n",
    "        print(f\"LR reductions: {lr_reductions}\")\n",
    "        \n",
    "        # Show LR progression if you want detail\n",
    "        if len(best_results['lr_progression']) > 1:\n",
    "            print(f\"LR progression: {best_results['lr_progression'][0]:.2e} → {final_lr:.2e}\")\n",
    "        # Make predictions\n",
    "        print(\"Making predictions on test data...\")\n",
    "        predictions = operational_model.predict(X_test_array, batch_size=32, verbose=1)\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        print(\"Evaluating model on test data...\")\n",
    "        test_metrics = operational_model.evaluate(X_test_array, y_test_array, batch_size=32, verbose=1)\n",
    "\n",
    "        # Print test metrics\n",
    "        print(f\"\\n{group_name} Operational Results (Adapted to 2025):\")\n",
    "        print(f\"Masked MSE (Loss): {test_metrics[0]:.4f}\")\n",
    "        print(f\"Masked RMSE:       {test_metrics[1]:.4f}\")\n",
    "        print(f\"Masked MAE:        {test_metrics[2]:.4f}\")\n",
    "        print(f\"Masked MSE Metric: {test_metrics[3]:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Baseline Comparison (No Adaptation) ---\")\n",
    "        baseline_model = model_implementation(featNo, architecture, final_activation)\n",
    "        baseline_model.load_weights(best_weights_path)\n",
    "        baseline_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=custom_loss_fn,\n",
    "            metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "        )\n",
    "        \n",
    "        # Process predictions and save\n",
    "        for i, pred in enumerate(predictions): # CHECK TO SEE IF THIS IS THE CORRECT SYNTAX\n",
    "            array = pred.reshape((256, 256))\n",
    "            mask = y_test_g[i].reshape((256, 256)) != -1\n",
    "            array_masked = np.where(mask, array, -1)\n",
    "            \n",
    "            save_array_as_raster(\n",
    "                output_path=f\"{out_folder}/prediction_{i}.tif\",\n",
    "                array=array_masked.astype(np.float32),\n",
    "                extent=g_test_extents[i],\n",
    "                crs=g_test_crs[i],\n",
    "                nodata_val=-1\n",
    "            )\n",
    "            \n",
    "            del array, mask, array_masked\n",
    "            \n",
    "        adaptation_metrics = {\n",
    "            'Group': [group_name],\n",
    "            'Baseline_RMSE': [baseline_stats['RMSE'][0]],\n",
    "            'Adapted_RMSE': [test_metrics[1]],\n",
    "            'Adapted_MSE': [test_metrics[3]],\n",
    "            'Adapted_MAE': [test_metrics[2]],\n",
    "            'Improvement': [baseline_stats['RMSE'][0] - test_metrics[1]],\n",
    "            'Adaptation_Epochs': [adaptation_epochs],\n",
    "            'Final_Val_Loss': [best_results['best_val_loss']],\n",
    "            'Initial_LR': [learningRateTesting],\n",
    "            'Final_LR': [final_lr],\n",
    "            'LR_Reductions': [lr_reductions],\n",
    "            'Overfitting_Gap': [overfitting_gap],\n",
    "            'Optimal_Freeze_Level': [best_freezing_level],\n",
    "            'Frozen_Layers': [frozen_count],\n",
    "            'Trainable_Layers': [trainable_count]   \n",
    "        }\n",
    "                \n",
    "        adaptation_df = pd.DataFrame(adaptation_metrics)\n",
    "        adaptation_csv = inter_model_outWorkspace + f\"operational_adaptation_results.csv\"\n",
    "        \n",
    "        if os.path.exists(adaptation_csv):\n",
    "            adaptation_df.to_csv(adaptation_csv, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            adaptation_df.to_csv(adaptation_csv, index=False)\n",
    "            \n",
    "        # Cleanup\n",
    "        del operational_model, baseline_model, custom_loss_fn\n",
    "        del X_train_g, y_train_g, g_train_extents, g_train_crs\n",
    "        del X_test_g, y_test_g, g_test_extents, g_test_crs\n",
    "        del X_train_array, y_train_array, X_test_array, y_test_array\n",
    "        del predictions, test_metrics\n",
    "        # del adaptation_history  # Remove this line\n",
    "        \n",
    "        clear_memory()\n",
    "        print(f\"{group_name} operational testing completed\\n\")\n",
    "        \n",
    "        # End timing for this architecture\n",
    "        end_time = time.time()\n",
    "        total_minutes = (end_time - start_time) / 60\n",
    "        print(f\"\\nArchitecture {architecture} completed in {total_minutes:.2f} minutes\")\n",
    "\n",
    "    try:\n",
    "        f.close()\n",
    "        sys.stdout = sys.__stdout__  # Reset stdout\n",
    "    except:\n",
    "        pass\n",
    "    clear_memory()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

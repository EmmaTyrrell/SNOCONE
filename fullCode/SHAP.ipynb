{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf5f746-7a38-4069-8d26-77dc16601174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import sys\n",
    "sys.path.append(\"D:/ASOML/SNOCONE\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from CNN_benchmarks import*\n",
    "from CNN_memoryOptimization import*\n",
    "from CNN_preProcessing import*\n",
    "from CNN_benchmarks import*\n",
    "from CNN_modelArchitectureBlocks import*\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import threading\n",
    "final_activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990f923e-afd3-4114-8842-eddc89054864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shap_optimized_gpu_cpu(weights_path, X_sample, feature_names, featNo, architecture, final_activation, custom_loss_fn, output_dir=None):\n",
    "    \"\"\"\n",
    "    Optimized SHAP analysis using both GPU and CPU efficiently\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import gc\n",
    "    import psutil\n",
    "    import matplotlib.pyplot as plt\n",
    "    import multiprocessing as mp\n",
    "    from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "    import threading\n",
    "    \n",
    "    def monitor_resources():\n",
    "        memory = psutil.virtual_memory()\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        gpu_info = \"\"\n",
    "        try:\n",
    "            import GPUtil\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]\n",
    "                gpu_info = f\"GPU: {gpu.memoryUtil*100:.1f}% memory, {gpu.load*100:.1f}% load\"\n",
    "        except:\n",
    "            gpu_info = \"GPU info unavailable\"\n",
    "        \n",
    "        print(f\"Memory: {memory.percent:.1f}% ({memory.used/1024**3:.1f}GB/{memory.total/1024**3:.1f}GB)\")\n",
    "        print(f\"CPU: {cpu_percent:.1f}% | {gpu_info}\")\n",
    "    \n",
    "    def configure_gpu():\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                # Set memory growth - this is the most important setting\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                \n",
    "                print(f\"GPU configured: {len(gpus)} device(s) with memory growth enabled\")\n",
    "                return True\n",
    "            except RuntimeError as e:\n",
    "                print(f\"GPU setup error: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"No GPUs found\")\n",
    "            return False\n",
    "    \n",
    "    # Configure CPU for parallel processing\n",
    "    def configure_cpu():\n",
    "        # Set CPU threads for TensorFlow\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(0)  # Use all available cores\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(0)  # Use all available cores\n",
    "        \n",
    "        # Get optimal number of CPU cores to use\n",
    "        cpu_cores = mp.cpu_count()\n",
    "        optimal_threads = max(1, cpu_cores - 2)  # Leave 2 cores for system\n",
    "        \n",
    "        print(f\"CPU configured: {cpu_cores} cores available, using {optimal_threads} threads\")\n",
    "        return optimal_threads\n",
    "    \n",
    "    print(\"Initial resources:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Configure hardware\n",
    "    gpu_available = configure_gpu()\n",
    "    cpu_threads = configure_cpu()\n",
    "    \n",
    "    # Intelligent sample sizing based on available resources\n",
    "    if gpu_available:\n",
    "        max_background = 10   # Reduced for memory efficiency\n",
    "        max_explain = 5       # Reduced for memory efficiency\n",
    "        batch_size = 1        # Process one at a time\n",
    "    else:\n",
    "        max_background = 5    # Fewer samples for CPU-only\n",
    "        max_explain = 3       # Fewer samples for CPU-only\n",
    "        batch_size = 1        # Smaller batches for CPU-only\n",
    "    \n",
    "    # CPU-based data preprocessing (parallel)\n",
    "    def preprocess_data_parallel(X_data, max_samples, name):\n",
    "        print(f\"Preprocessing {name} data using CPU...\")\n",
    "        \n",
    "        if len(X_data) > max_samples:\n",
    "            # Use CPU for random sampling\n",
    "            indices = np.random.choice(len(X_data), max_samples, replace=False)\n",
    "            result = X_data[indices]\n",
    "        else:\n",
    "            result = X_data[:max_samples]\n",
    "        \n",
    "        # CPU-based data type conversion\n",
    "        result = result.astype(np.float32)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Process background and explain data in parallel using CPU\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        bg_future = executor.submit(preprocess_data_parallel, X_sample, max_background, \"background\")\n",
    "        exp_future = executor.submit(preprocess_data_parallel, X_sample, max_explain, \"explain\")\n",
    "        \n",
    "        background = bg_future.result()\n",
    "        X_explain = exp_future.result()\n",
    "    \n",
    "    print(f\"Background samples: {len(background)}, Explain samples: {len(X_explain)}\")\n",
    "    \n",
    "    # ADD DOWNSAMPLING FUNCTION\n",
    "    def downsample_for_shap(X_data, target_size=(128, 128)):\n",
    "        \"\"\"Downsample images for SHAP to reduce memory usage\"\"\"\n",
    "        print(f\"Downsampling from {X_data.shape[1:3]} to {target_size}\")\n",
    "        X_resized = tf.image.resize(X_data, target_size)\n",
    "        return X_resized.numpy()\n",
    "    \n",
    "    # Apply downsampling before SHAP\n",
    "    print(\"Downsampling images for SHAP...\")\n",
    "    original_shape = background.shape\n",
    "    background = downsample_for_shap(background, (128, 128))\n",
    "    X_explain = downsample_for_shap(X_explain, (128, 128))\n",
    "    print(f\"Downsampled from {original_shape[1:3]} to {background.shape[1:3]}\")\n",
    "    \n",
    "    # CREATE WRAPPER MODEL FUNCTION\n",
    "    def create_downsampled_model(original_model, original_size=(256, 256), target_size=(128, 128)):\n",
    "        \"\"\"Create a model that accepts downsampled inputs but uses original model\"\"\"\n",
    "        \n",
    "        # Create input layer for downsampled size\n",
    "        downsampled_input = tf.keras.layers.Input(shape=(*target_size, featNo))\n",
    "        \n",
    "        # Upsample back to original size for the model\n",
    "        upsampled = tf.image.resize(downsampled_input, original_size)\n",
    "        \n",
    "        # Pass through original model\n",
    "        output = original_model(upsampled)\n",
    "        \n",
    "        # Create new model\n",
    "        wrapper_model = tf.keras.Model(inputs=downsampled_input, outputs=output)\n",
    "        return wrapper_model\n",
    "    \n",
    "    # Load model on GPU if available\n",
    "    print(\"Loading model...\")\n",
    "    with tf.device('/GPU:0' if gpu_available else '/CPU:0'):\n",
    "        original_model = model_implementation(featNo, architecture, final_activation)\n",
    "        original_model.load_weights(weights_path)\n",
    "        original_model.compile(optimizer='adam', loss=custom_loss_fn, metrics=[masked_rmse, masked_mae, masked_mse])\n",
    "        \n",
    "        # Create wrapper model for downsampled inputs\n",
    "        model = create_downsampled_model(original_model, (256, 256), (128, 128))\n",
    "        print(\"Created wrapper model for downsampled inputs\")\n",
    "    \n",
    "    print(\"After model loading:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # Create explainer with GPU/CPU optimization\n",
    "    print(\"Creating SHAP GradientExplainer...\")\n",
    "    \n",
    "    try:\n",
    "        # GradientExplainer expects the model object, not a function\n",
    "        explainer = shap.GradientExplainer(model, background)\n",
    "        \n",
    "        print(\"After explainer creation:\")\n",
    "        monitor_resources()\n",
    "        \n",
    "        # Optimized SHAP calculation using both GPU and CPU\n",
    "        print(\"Calculating SHAP values...\")\n",
    "        all_shap_values = []\n",
    "        \n",
    "        # Process in optimal batches\n",
    "        for i in range(0, len(X_explain), batch_size):\n",
    "            batch_end = min(i + batch_size, len(X_explain))\n",
    "            batch = X_explain[i:batch_end]\n",
    "            \n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(len(X_explain)-1)//batch_size + 1}\")\n",
    "            \n",
    "            # Use GPU for SHAP calculation, CPU for pre/post processing\n",
    "            try:\n",
    "                with tf.device('/GPU:0' if gpu_available else '/CPU:0'):\n",
    "                    shap_val = explainer.shap_values(batch, nsamples=10)  # Reduced nsamples\n",
    "                \n",
    "                if isinstance(shap_val, list):\n",
    "                    shap_val = shap_val[0]\n",
    "                \n",
    "                all_shap_values.append(shap_val)\n",
    "                \n",
    "                # Monitor every few batches\n",
    "                if i % 2 == 0:\n",
    "                    monitor_resources()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # CPU-based result combination\n",
    "        if all_shap_values:\n",
    "            print(\"Combining results using CPU...\")\n",
    "            shap_values = np.concatenate(all_shap_values, axis=0)\n",
    "        else:\n",
    "            raise Exception(\"No SHAP values calculated successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Primary SHAP calculation failed: {e}\")\n",
    "        print(\"Trying fallback with minimal samples...\")\n",
    "        \n",
    "        tiny_background = background[:2]  # Only 2 background samples\n",
    "        tiny_explain = X_explain[:1]      # Only 1 explain sample\n",
    "        \n",
    "        with tf.device('/CPU:0'):  # Force CPU\n",
    "            explainer = shap.GradientExplainer(model, tiny_background)\n",
    "            shap_values = explainer.shap_values(tiny_explain, nsamples=5) \n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "    \n",
    "    print(f\"Final SHAP values shape: {shap_values.shape}\")\n",
    "    print(\"After SHAP calculation:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    # CPU-based feature importance calculation (parallel)\n",
    "    def calculate_feature_importance_parallel(shap_vals):\n",
    "        print(\"Calculating feature importance using CPU...\")\n",
    "        \n",
    "        if len(shap_vals.shape) == 4:  # (samples, height, width, features)\n",
    "            # Use CPU for numerical computation\n",
    "            importance = np.mean(np.abs(shap_vals), axis=(0, 1, 2))\n",
    "        else:\n",
    "            importance = np.mean(np.abs(shap_vals), axis=0)\n",
    "        \n",
    "        return importance\n",
    "    \n",
    "    # Calculate feature importance on CPU\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        importance_future = executor.submit(calculate_feature_importance_parallel, shap_values)\n",
    "        feature_importance = importance_future.result()\n",
    "    \n",
    "    # CPU-based results processing\n",
    "    results = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP_Importance': feature_importance,\n",
    "        'Normalized_Importance': feature_importance / np.max(feature_importance)\n",
    "    }).sort_values('SHAP_Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    results['Rank'] = range(1, len(results) + 1)\n",
    "    \n",
    "    print(\"\\nFeature Importance Rankings (GPU+CPU Optimized):\")\n",
    "    print(results[['Rank', 'Feature', 'SHAP_Importance']].to_string(index=False))\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del original_model, model, explainer, shap_values\n",
    "    if gpu_available:\n",
    "        tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    # CPU-based file I/O and plotting\n",
    "    if output_dir is not None:\n",
    "        print(\"Saving results using CPU...\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CSV using CPU\n",
    "        csv_path = os.path.join(output_dir, 'feature_importance_gpu_cpu_optimized.csv')\n",
    "        results.to_csv(csv_path, index=False)\n",
    "        print(f\"CSV saved: {csv_path}\")\n",
    "        \n",
    "        # CPU-based plotting\n",
    "        def create_plot():\n",
    "            try:\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "                top_features = results.head(15)\n",
    "                \n",
    "                colors = plt.cm.viridis(top_features['Normalized_Importance'])\n",
    "                bars = ax.barh(range(len(top_features)), top_features['SHAP_Importance'], color=colors)\n",
    "                ax.set_yticks(range(len(top_features)))\n",
    "                ax.set_yticklabels(top_features['Feature'])\n",
    "                ax.set_xlabel('SHAP Importance (GPU+CPU Optimized)')\n",
    "                ax.set_title('Top 15 SWE Feature Importance (GPU+CPU Optimized)')\n",
    "                ax.invert_yaxis()\n",
    "                \n",
    "                for i, bar in enumerate(bars):\n",
    "                    width = bar.get_width()\n",
    "                    ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                           f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                plot_path = os.path.join(output_dir, 'feature_importance_gpu_cpu_plot.png')\n",
    "                plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Plot saved: {plot_path}\")\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Plotting failed: {e}\")\n",
    "        \n",
    "        # Run plotting on CPU\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            plot_future = executor.submit(create_plot)\n",
    "            plot_future.result()\n",
    "    \n",
    "    print(\"Final resources:\")\n",
    "    monitor_resources()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b197f03-f279-495c-b8cc-ea302cdfec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fSCA', 'DMFSCA', 'DOY', 'Tree Density', 'LandCover', 'ASOML_CON_waterbodies_1_0_60_albn83', 'ASOML_CON_windScour_60_albn83_scl', 'ASO_CON_aspect_albn83_60m_scl', 'ASO_CON_curv_albn83_60m_scl', 'ASO_CON_DaH_albn83_60m_scl', 'ASO_CON_dem_albn83_60m_scl', 'ASO_CON_GausCurv_albn83_60m_scl', 'ASO_CON_gradMag_60_albn83_scl', 'ASO_CON_lat_albn83_60m_scl', 'ASO_CON_lon_albn83_60m_scl', 'ASO_CON_PlanCurv_albn83_60m_scl', 'ASO_CON_ProCurv_albn83_60m_scl', 'ASO_CON_slope_albn83_60m_scl', 'ASO_CON_stdElv_60_albn83_scl', 'ASO_CON_STDslope_albn83_60m_scl', 'ASO_CON_TPI_albn83_60m_scl', 'ASO_CON_TRASP_albn83_60m_scl']\n",
      "Conejos\n",
      "Processing year 2022\n",
      "Processing year 2023\n",
      "Processing year 2024\n",
      "Preprocessing data for memory efficiency...\n",
      "['fSCA', 'DMFSCA', 'DOY', 'Tree Density', 'LandCover', 'ASOML_CON_waterbodies_1_0_60_albn83', 'ASOML_CON_windScour_60_albn83_scl', 'ASO_CON_aspect_albn83_60m_scl', 'ASO_CON_curv_albn83_60m_scl', 'ASO_CON_DaH_albn83_60m_scl', 'ASO_CON_dem_albn83_60m_scl', 'ASO_CON_GausCurv_albn83_60m_scl', 'ASO_CON_gradMag_60_albn83_scl', 'ASO_CON_lat_albn83_60m_scl', 'ASO_CON_lon_albn83_60m_scl', 'ASO_CON_PlanCurv_albn83_60m_scl', 'ASO_CON_ProCurv_albn83_60m_scl', 'ASO_CON_slope_albn83_60m_scl', 'ASO_CON_stdElv_60_albn83_scl', 'ASO_CON_STDslope_albn83_60m_scl', 'ASO_CON_TPI_albn83_60m_scl', 'ASO_CON_TRASP_albn83_60m_scl']\n",
      "Reducing sample size from 143 to 100\n",
      "Final sample shapes: X=(100, 256, 256, 22), y=(100, 65536)\n",
      "Memory usage: X=0.54GB, y=0.02GB\n",
      "Initial resources:\n",
      "Memory: 44.8% (14.2GB/31.6GB)\n",
      "CPU: 7.7% | GPU: 0.0% memory, 0.0% load\n",
      "GPU configured: 1 device(s) with memory growth enabled\n",
      "CPU configured: 22 cores available, using 20 threads\n",
      "Preprocessing background data using CPU...\n",
      "Preprocessing explain data using CPU...\n",
      "Background samples: 10, Explain samples: 5\n",
      "Downsampling images for SHAP...\n",
      "Downsampling from (256, 256) to (128, 128)\n",
      "Downsampling from (256, 256) to (128, 128)\n",
      "Downsampled from (256, 256) to (128, 128)\n",
      "Loading model...\n",
      "Created wrapper model for downsampled inputs\n",
      "After model loading:\n",
      "Memory: 46.0% (14.5GB/31.6GB)\n",
      "CPU: 4.8% | GPU: 18.0% memory, 0.0% load\n",
      "Creating SHAP GradientExplainer...\n",
      "After explainer creation:\n",
      "Memory: 46.0% (14.5GB/31.6GB)\n",
      "CPU: 7.6% | GPU: 18.0% memory, 0.0% load\n",
      "Calculating SHAP values...\n",
      "Processing batch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _TFGradient.gradient.<locals>.grad_graph at 0x000001CFBA2A29E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _TFGradient.gradient.<locals>.grad_graph at 0x000001CFBA2A3010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# establish parameters\n",
    "Domain = \"Rockies\"\n",
    "basin_name = \"Conejos\"\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "ModelOutputs = f\"{WorkspaceBase}/modelOutputs/BasinSpecifics/\"\n",
    "time_code = \"20250716_121023\"\n",
    "model_interation = f\"{basin_name}_{time_code}\"\n",
    "feature_Listcsv = f\"{ModelOutputs}/{Domain}_{basin_name}_model_featureList_summary.csv\"\n",
    "best_weights = ModelOutputs + f\"/{model_interation}/best_weights_{model_interation}.h5\"\n",
    "start_year = 2022\n",
    "end_year = 2024\n",
    "shap_output = f\"{ModelOutputs}/{model_interation}/shap_results/\"\n",
    "architecture = \"AdvancedBaseline\"\n",
    "shapeChecks = \"N\"\n",
    "\n",
    "# workspaces\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "DMFSCAWorkspace = WorkspaceBase + \"Rockies_DMFSCA/\"\n",
    "\n",
    "nonFreezeLayers = -3\n",
    "learningRateTesting = 1e-4\n",
    "penalty_weight = 0.6\n",
    "penalties_used = [\"fSCA\", \"LowSnow\"]\n",
    "low_snow_weight=0.2    \n",
    "swe_threshold=0.01\n",
    "fsca_threshold=0.01\n",
    "low_threshold=0.05\n",
    "penalty_scale=2.0\n",
    "\n",
    "## get list of features\n",
    "feat_df = pd.read_csv(feature_Listcsv)\n",
    "feat_names = feat_df[[f'{time_code}']].dropna().astype(str)\n",
    "featNo = len(feat_df[[f'{time_code}']].dropna().astype(str))\n",
    "feature_names = feat_names[f'{time_code}'].dropna().astype(str).tolist()\n",
    "print(feature_names)\n",
    "\n",
    "print(basin_name)\n",
    "X_sample, y_sample, featureNames = target_feature_stacks_basins(start_year=start_year, \n",
    "                                                   end_year=end_year, \n",
    "                                                   WorkspaceBase=WorkspaceBase, \n",
    "                                                   ext = \"nonull_fnl.tif\", \n",
    "                                                   vegetation_path = tree_workspace, \n",
    "                                                   landCover_path = land_workspace, \n",
    "                                                   phv_path = phv_features, \n",
    "                                                   target_shape=(256,256),\n",
    "                                                   basin_name=basin_name,\n",
    "                                                   shapeChecks=\"Y\")\n",
    "\n",
    "print(\"Preprocessing data for memory efficiency...\")\n",
    "print(featureNames)\n",
    "featNo = len(featureNames)\n",
    "\n",
    "# Reduce sample size if too large\n",
    "if len(X_sample) > 100:\n",
    "    print(f\"Reducing sample size from {len(X_sample)} to 100\")\n",
    "    sample_indices = np.random.choice(len(X_sample), 100, replace=False)\n",
    "    X_sample = X_sample[sample_indices]\n",
    "    y_sample = y_sample[sample_indices]\n",
    "\n",
    "# Convert to float32 to save memory\n",
    "X_sample = X_sample.astype(np.float32)\n",
    "y_sample = y_sample.astype(np.float32)\n",
    "\n",
    "print(f\"Final sample shapes: X={X_sample.shape}, y={y_sample.shape}\")\n",
    "print(f\"Memory usage: X={X_sample.nbytes/1024**3:.2f}GB, y={y_sample.nbytes/1024**3:.2f}GB\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Create your loss function first\n",
    "custom_loss_fn = make_combined_swe_fsca_lowsnow_loss(\n",
    "                base_loss_fn=MeanSquaredError(),\n",
    "                penalty_weight=penalty_weight,   \n",
    "                low_snow_weight=low_snow_weight,            \n",
    "                swe_threshold=swe_threshold,\n",
    "                fsca_threshold=fsca_threshold,\n",
    "                low_threshold=low_threshold,\n",
    "                penalty_scale=penalty_scale,\n",
    "                mask_value=-1\n",
    "            )\n",
    "\n",
    "# Run the optimized SHAP analysis\n",
    "results = run_shap_optimized_gpu_cpu(weights_path=best_weights, \n",
    "                                     X_sample=X_sample, \n",
    "                                     feature_names=featureNames, \n",
    "                                     featNo=featNo, \n",
    "                                     architecture=architecture, \n",
    "                                     final_activation=\"relu\", \n",
    "                                     custom_loss_fn=custom_loss_fn, \n",
    "                                     output_dir=shap_output)\n",
    "\n",
    "print(\"\\n=== SHAP Analysis Complete ===\")\n",
    "print(f\"Results saved to: {shap_output}\")\n",
    "print(f\"Top 3 most important features:\")\n",
    "for i in range(min(3, len(results))):\n",
    "    print(f\"{i+1}. {results.iloc[i]['Feature']}: {results.iloc[i]['SHAP_Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89660e-f94c-43a2-8908-d645081a70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "print(\"=== WEIGHTS FILE ANALYSIS ===\")\n",
    "with h5py.File(best_weights, 'r') as f:\n",
    "    print(f\"Weights file path: {best_weights}\")\n",
    "    print(\"Layer names in weights file:\")\n",
    "    layer_names = []\n",
    "    for key in f.keys():\n",
    "        print(f\"  {key}\")\n",
    "        layer_names.append(key)\n",
    "    print(f\"Total layers in file: {len(layer_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020a9cb-3145-4843-872a-c7b3c3fc4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== CURRENT MODEL ANALYSIS ===\")\n",
    "model = model_implementation(featNo, architecture, final_activation)\n",
    "print(f\"Current model layers: {len(model.layers)}\")\n",
    "print(\"Current model layer names:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"  {i}: {layer.name} ({type(layer).__name__})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e7e55-f15f-49ee-a979-9a54a4a1f8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

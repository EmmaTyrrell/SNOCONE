{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c700c-1ac3-412d-bb4c-5488b26bf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseduo code\n",
    "import sys\n",
    "sys.path.append(\"D:/ASOML/SNOCONE\")\n",
    "from CNN_memoryOptimization import*\n",
    "from CNN_preProcessing import*\n",
    "from CNN_benchmarks import* \n",
    "from CNN_modelArchitectureBlocks import* \n",
    "import rasterio\n",
    "import shap\n",
    "import pandas as pd\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "import psutil\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.losses import Loss\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "# years = list(range(2022, 2023))\n",
    "start_year = 2022\n",
    "end_year = 2024\n",
    "test_start_year = 2025\n",
    "test_end_year =2025\n",
    "Domain = \"Rockies\"\n",
    "GPU = \"N\"\n",
    "\n",
    "# workspaces\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/BasinSpecifics/\"\n",
    "DMFSCAWorkspace = WorkspaceBase + \"Rockies_DMFSCA/\"\n",
    "final_activation = 'relu'\n",
    "architectures = ['AdvancedBaseline']  # Options are: Baseline, ResNet18, ResNet34, ResNet50, CustomSWE\n",
    "basin_list = ['Conejos', 'Dolores', 'EastRiver', 'BoulderCreek', 'ClearCreek', 'SouthPlatte', 'WindyGap']\n",
    "\n",
    "nonFreezeLayers = -3\n",
    "learningRateTesting = 1e-4\n",
    "penalty_weight = 0.6\n",
    "penalties_used = [\"fSCA\", \"LowSnow\"]\n",
    "low_snow_weight=0.2    \n",
    "swe_threshold=0.01\n",
    "fsca_threshold=0.01\n",
    "low_threshold=0.05\n",
    "penalty_scale=2.0\n",
    "\n",
    "test_groups = [\n",
    "            ('Group1', 2025, 'G1'),\n",
    "            ('Group2', 2025, 'G2'), \n",
    "            ('Group3', 2025, 'G3'),\n",
    "            ('Group4', 2025, 'G4'),\n",
    "            ('Group5', 2025, 'G5'),\n",
    "            ('Group6', 2025, 'G6')\n",
    "        ]\n",
    "\n",
    "_mae_metric = MeanAbsoluteError()\n",
    "_mse_metric = MeanSquaredError()\n",
    "_rmse_metric = MeanSquaredError()\n",
    "\n",
    "# Check GPU availability\n",
    "if GPU == \"Y\":\n",
    "    print(\"\\n Checking and start running on GPU\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Configure GPU memory growth (prevents TensorFlow from allocating all GPU memory)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Verify TensorFlow is using GPU\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"GPU computation result:\", c)\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for basin_name in basin_list:\n",
    "    for architecture in architectures:\n",
    "        print(architecture)\n",
    "        # shapeChecks = \"N\"\n",
    "        ## seting folder\n",
    "        \n",
    "        # create folder for model outputs\n",
    "        os.makedirs(modelOuptuts + f\"{basin_name}_{str(timestamp)}/\", exist_ok=True)\n",
    "        inter_model_outWorkspace = modelOuptuts + f\"{basin_name}_{str(timestamp)}/\"\n",
    "        \n",
    "        f = open(inter_model_outWorkspace + f\"code_output_{basin_name}_{timestamp}.txt\", \"a\")\n",
    "        sys.stdout = f\n",
    "        print(\"MODEL OUTPUTS TO BE PRINTED TO THIS DOC\")\n",
    "        print(f\"\\n {basin_name} is being processed and tested\")\n",
    "        X, y, featureName = target_feature_stacks_basins(start_year=start_year, \n",
    "                                                   end_year=end_year, \n",
    "                                                   WorkspaceBase=WorkspaceBase, \n",
    "                                                   ext = \"nonull_fnl.tif\", \n",
    "                                                   vegetation_path = tree_workspace, \n",
    "                                                   landCover_path = land_workspace, \n",
    "                                                   phv_path = phv_features , \n",
    "                                                   target_shape=(256,256),\n",
    "                                                   basin_name=basin_name,\n",
    "                                                   shapeChecks=\"Y\")\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Shape of input data\")\n",
    "        print(f\"feature shape: {X.shape}\")\n",
    "        print(f\"target shape: {y.shape}\")\n",
    "        feat_shape = X.shape\n",
    "        featNo = feat_shape[-1]\n",
    "        if X.shape[0] == 0:\n",
    "            print(\"no basins for training\")\n",
    "            continue\n",
    "        else: \n",
    "            print(featNo)\n",
    "            print(featureName)\n",
    "            \n",
    "            # split between training and test data\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "            print(\"***\")\n",
    "            print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "            print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "            print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "            print(\"***\")\n",
    "            \n",
    "            x_trainShape = X_train.shape\n",
    "            x_validShape = X_valid.shape\n",
    "            \n",
    "            # Assuming featNo and final_activation are defined in your original code\n",
    "            featNo = featNo  # Replace with your actual feature count\n",
    "            final_activation = final_activation  # Replace with your actual activation\n",
    "            \n",
    "            # Create the model\n",
    "            model = model_implementation(featNo, architecture, final_activation)\n",
    "            \n",
    "            # Your existing custom loss function\n",
    "            from tensorflow.keras.losses import MeanSquaredError\n",
    "            from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "            \n",
    "            custom_loss_fn = make_combined_swe_fsca_lowsnow_loss(\n",
    "                base_loss_fn=MeanSquaredError(),\n",
    "                penalty_weight=penalty_weight,   \n",
    "                low_snow_weight=low_snow_weight,            \n",
    "                swe_threshold=swe_threshold,\n",
    "                fsca_threshold=fsca_threshold,\n",
    "                low_threshold=low_threshold,\n",
    "                penalty_scale=penalty_scale,\n",
    "                mask_value=-1\n",
    "            )\n",
    "                    \n",
    "            # Compile with your existing setup\n",
    "            model.compile(\n",
    "                optimizer='adam',\n",
    "                loss=custom_loss_fn,\n",
    "                metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "            )\n",
    "            \n",
    "            print(model.summary())\n",
    "            \n",
    "            # establish the model\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                f\"{inter_model_outWorkspace}/best_weights_{basin_name}_{timestamp}.h5\", \n",
    "                monitor=\"val_loss\",\n",
    "                verbose=1, \n",
    "                save_best_only=True, \n",
    "                mode='min'\n",
    "            )\n",
    "            early_stopping = EarlyStopping(monitor=\"val_masked_rmse\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "            \n",
    "            \n",
    "            batch_size = 8\n",
    "            train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "            valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "            \n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                validation_data=valid_generator,\n",
    "                epochs=100,\n",
    "                callbacks=[checkpoint, early_stopping]\n",
    "            )\n",
    "        \n",
    "            ## plotting results \n",
    "            fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "            # 1. Masked MSE (used as loss)\n",
    "            axs[0].plot(history.history['loss'], label='Train MSE Loss')\n",
    "            axs[0].plot(history.history['val_loss'], label='Val MSE Loss')\n",
    "            axs[0].set_ylabel('MSE Loss')\n",
    "            axs[0].set_title('Masked MSE')\n",
    "            axs[0].legend()\n",
    "            axs[0].grid(True)\n",
    "            # 2. Masked RMSE\n",
    "            axs[1].plot(history.history['masked_rmse'], label='Train RMSE')\n",
    "            axs[1].plot(history.history['val_masked_rmse'], label='Val RMSE')\n",
    "            axs[1].set_ylabel('RMSE')\n",
    "            axs[1].set_title('Masked RMSE')\n",
    "            axs[1].legend()\n",
    "            axs[1].grid(True)\n",
    "            # 3. Masked MAE\n",
    "            axs[2].plot(history.history['masked_mae'], label='Train MAE')\n",
    "            axs[2].plot(history.history['val_masked_mae'], label='Val MAE')\n",
    "            axs[2].set_ylabel('MAE')\n",
    "            axs[2].set_title('Masked MAE')\n",
    "            axs[2].set_xlabel('Epoch')\n",
    "            axs[2].legend()\n",
    "            axs[2].grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(inter_model_outWorkspace + f\"{basin_name}_Model_error_epochs.png\")\n",
    "            plt.show()\n",
    "            \n",
    "            ## Add metrics to recurring error tracking sheet\n",
    "            metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "            best_metrics = {}\n",
    "            print(\"\\nValidation Metric Progression:\")\n",
    "            for metric in metrics_to_track:\n",
    "                values = history.history.get(metric, [])\n",
    "                if values:\n",
    "                    best_val = min(values)\n",
    "                    best_metrics[metric] = best_val\n",
    "                    print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "                    print(f\"{metric}: Best = {best_val:.4f}\")\n",
    "                else:\n",
    "                    print(f\"{metric}: Not found in history.\")\n",
    "            print(f\"Final activation function: {final_activation}\")\n",
    "            \n",
    "            # Example variables\n",
    "            feature_csv = modelOuptuts + f\"{Domain}_{basin_name}_model_featureList_summary.csv\" \n",
    "            column_name = timestamp  \n",
    "            feature_list = featureName   \n",
    "            new_column_df = pd.DataFrame({column_name: feature_list})\n",
    "            \n",
    "            # If the file already exists, load it and append the new column\n",
    "            if os.path.exists(feature_csv):\n",
    "                existing_df = pd.read_csv(feature_csv)\n",
    "                # Reindex the existing dataframe \n",
    "                max_len = max(len(existing_df), len(new_column_df))\n",
    "                existing_df = existing_df.reindex(range(max_len))\n",
    "                new_column_df = new_column_df.reindex(range(max_len))\n",
    "                # Combine horizontally\n",
    "                combined_df = pd.concat([existing_df, new_column_df], axis=1)\n",
    "            else:\n",
    "                combined_df = new_column_df\n",
    "            \n",
    "            # Save back to CSV\n",
    "            combined_df.to_csv(feature_csv, index=False)\n",
    "            \n",
    "            # add metrics to csv\n",
    "            modelStatsCSV = modelOuptuts + f\"{Domain}_{basin_name}_modelSummary_stats.csv\"\n",
    "            \n",
    "            error_stats = {\n",
    "                'ModelRun':[timestamp],\n",
    "                'FeatureNum': [featNo],\n",
    "                'Architecture': [architecture],\n",
    "                'FinalActivation': [final_activation],\n",
    "                'X_TrainShape': [x_trainShape[0]],\n",
    "                'X_ValidShape': [x_validShape[0]],\n",
    "                'RMSE': [best_metrics['val_masked_rmse']],\n",
    "                'MSE': [best_metrics['val_masked_mse']], \n",
    "                'MAE': [best_metrics['val_masked_mae']]\n",
    "                \n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(error_stats)\n",
    "            \n",
    "            # Append or write new file\n",
    "            if os.path.exists(modelStatsCSV):\n",
    "                df.to_csv(modelStatsCSV, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                df.to_csv(modelStatsCSV, index=False)\n",
    "            \n",
    "            del X_train, X_valid, y_train, y_valid, X, y\n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "        \n",
    "            del history \n",
    "            clear_memory()\n",
    "    \n",
    "            ## apply to test groups\n",
    "            X_test, y_test, featureName = target_feature_stacks_basins(start_year=test_start_year, \n",
    "                                                       end_year=test_end_year, \n",
    "                                                       WorkspaceBase=f\"D:/ASOML/{Domain}/test_2025_basin/\", \n",
    "                                                       ext = \"nonull_fnl.tif\", \n",
    "                                                       vegetation_path = tree_workspace, \n",
    "                                                       landCover_path = land_workspace, \n",
    "                                                       phv_path = phv_features , \n",
    "                                                       target_shape=(256,256),\n",
    "                                                       basin_name=basin_name,\n",
    "                                                       shapeChecks=\"Y\")\n",
    "    \n",
    "            ## Apply model to test data\n",
    "            print(f\"\\n=== Applying trained model to test data ===\")\n",
    "            \n",
    "            # Load test data (X and y are already loaded from your code)\n",
    "            print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "            if X_test.size == 0:\n",
    "                continue\n",
    "            else: \n",
    "                # Create a new model with the same architecture\n",
    "                test_model = model_implementation(featNo, architecture, final_activation)\n",
    "                \n",
    "                # Load the best weights from training\n",
    "                best_weights_path = f\"{inter_model_outWorkspace}/best_weights_{basin_name}_{timestamp}.h5\"\n",
    "                test_model.load_weights(best_weights_path)\n",
    "                \n",
    "                # Compile the model (needed for evaluation metrics)\n",
    "                test_model.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss=custom_loss_fn,\n",
    "                    metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "                )\n",
    "                \n",
    "                # Make predictions\n",
    "                print(\"Making predictions on test data...\")\n",
    "                predictions = test_model.predict(X_test, batch_size=32, verbose=1)\n",
    "                \n",
    "                # Evaluate the model on test data\n",
    "                print(\"Evaluating model on test data...\")\n",
    "                test_metrics = test_model.evaluate(X_test, y_test, batch_size=32, verbose=1)\n",
    "                \n",
    "                # Print test metrics\n",
    "                metric_names = ['loss', 'masked_rmse', 'masked_mae', 'masked_mse']\n",
    "                print(f\"\\nTest Results for {basin_name}:\")\n",
    "                for i, metric_name in enumerate(metric_names):\n",
    "                    if i < len(test_metrics):\n",
    "                        print(f\"{metric_name}: {test_metrics[i]:.4f}\")\n",
    "                \n",
    "                # Save predictions\n",
    "                predictions_path = f\"{inter_model_outWorkspace}/{basin_name}_test_predictions.npy\"\n",
    "                np.save(predictions_path, predictions)\n",
    "                print(f\"Predictions saved to: {predictions_path}\")\n",
    "                \n",
    "                # Save actual test targets for comparison\n",
    "                actuals_path = f\"{inter_model_outWorkspace}/{basin_name}_test_actuals.npy\"\n",
    "                np.save(actuals_path, y_test)\n",
    "                print(f\"Test actuals saved to: {actuals_path}\")\n",
    "                \n",
    "                # Save test metrics to CSV\n",
    "                test_metrics_csv = modelOuptuts + f\"{Domain}_{basin_name}_testResults.csv\"\n",
    "                \n",
    "                test_results = {\n",
    "                    'ModelRun': [timestamp],\n",
    "                    'Basin': [basin_name],\n",
    "                    'Architecture': [architecture],\n",
    "                    'TestSamples': [X_test.shape[0]],\n",
    "                    'Test_Loss': [test_metrics[0] if len(test_metrics) > 0 else None],\n",
    "                    'Test_RMSE': [test_metrics[1] if len(test_metrics) > 1 else None],\n",
    "                    'Test_MAE': [test_metrics[2] if len(test_metrics) > 2 else None],\n",
    "                    'Test_MSE': [test_metrics[3] if len(test_metrics) > 3 else None]\n",
    "                }\n",
    "                \n",
    "                test_df = pd.DataFrame(test_results)\n",
    "                \n",
    "                # Append to existing CSV or create new one\n",
    "                if os.path.exists(test_metrics_csv):\n",
    "                    test_df.to_csv(test_metrics_csv, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    test_df.to_csv(test_metrics_csv, index=False)\n",
    "                \n",
    "                # Create visualizations comparing predictions vs actual\n",
    "                print(f\"Test evaluation completed for {basin_name}\")\n",
    "                print(f\"Results saved to: {inter_model_outWorkspace}\")\n",
    "                \n",
    "                # Clean up memory\n",
    "                del test_model, predictions\n",
    "                K.clear_session()\n",
    "                gc.collect()\n",
    "                \n",
    "                print(\"\\n=== Test data evaluation completed ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c1699-6351-4395-add2-607fcc6e89d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

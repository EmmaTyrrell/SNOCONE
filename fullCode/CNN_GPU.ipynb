{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98233f9c-abc6-4173-b5f2-0917fbbae6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c700c-1ac3-412d-bb4c-5488b26bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n",
      "modules established\n",
      "Baseline\n",
      "MODEL OUTPUTS TO BE PRINTED TO THIS DOC\n",
      "Processing year 2022\n"
     ]
    }
   ],
   "source": [
    "# pseduo code\n",
    "import sys\n",
    "sys.path.append(\"D:/ASOML/SNOCONE\")\n",
    "from CNN_memoryOptimization import clear_memory, memory_efficient_prediction, DataGenerator\n",
    "from CNN_preProcessing import min_max_scale, read_aligned_raster, save_array_as_raster, target_feature_stacks, target_feature_stacks_testGroups\n",
    "from CNN_benchmarks import swe_fsca_consistency_loss_fn, make_swe_fsca_loss, masked_loss_fn, masked_mse, masked_mae, masked_rmse\n",
    "from CNN_modelArchitectureBlocks import conv_block, identity_block, basic_block, resnet_model_implementation, model_predict, load_and_prepare_model, Baseline_CNN\n",
    "import rasterio\n",
    "import shap\n",
    "import pandas as pd\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "import psutil\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.losses import Loss\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "# years = list(range(2022, 2023))\n",
    "start_year = 2022\n",
    "end_year = 2022\n",
    "Domain = \"Rockies\"\n",
    "GPU = \"N\"\n",
    "\n",
    "# workspaces\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "DMFSCAWorkspace = WorkspaceBase + \"Rockies_DMFSCA/\"\n",
    "final_activation = 'relu'\n",
    "architectures = ['Baseline']  # Options are: Baseline, ResNet18, ResNet34, ResNet50, CustomSWE\n",
    "\n",
    "test_groups = [\n",
    "            ('Group1', 2025, 'G1'),\n",
    "            ('Group2', 2025, 'G2'), \n",
    "            ('Group3', 2025, 'G3'),\n",
    "            ('Group4', 2025, 'G4'),\n",
    "            ('Group5', 2025, 'G5'),\n",
    "            ('Group6', 2025, 'G6')\n",
    "        ]\n",
    "\n",
    "_mae_metric = MeanAbsoluteError()\n",
    "_mse_metric = MeanSquaredError()\n",
    "_rmse_metric = MeanSquaredError()\n",
    "\n",
    "# # Check GPU availability\n",
    "if GPU == \"Y\":\n",
    "    print(\"\\n Checking and start running on GPU\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Configure GPU memory growth (prevents TensorFlow from allocating all GPU memory)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Verify TensorFlow is using GPU\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"GPU computation result:\", c)\n",
    "\n",
    "for architecture in architectures:\n",
    "    print(architecture)\n",
    "    # shapeChecks = \"N\"\n",
    "    ## seting folder\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # create folder for model outputs\n",
    "    os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "    inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n",
    "    \n",
    "    # f = open(inter_model_outWorkspace + f\"code_output_{timestamp}.txt\", \"a\")\n",
    "    # sys.stdout = f\n",
    "    print(\"MODEL OUTPUTS TO BE PRINTED TO THIS DOC\")\n",
    "    X, y, featureNames = target_feature_stacks(start_year=start_year, \n",
    "                                               end_year=end_year, \n",
    "                                               WorkspaceBase=WorkspaceBase, \n",
    "                                               ext = \"nonull_fnl.tif\", \n",
    "                                               vegetation_path = tree_workspace, \n",
    "                                               landCover_path = land_workspace, \n",
    "                                               phv_path = phv_features , \n",
    "                                               target_shape=(256,256),\n",
    "                                               shapeChecks=\"Y\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Shape of input data\")\n",
    "    print(f\"feature shape: {X.shape}\")\n",
    "    print(f\"target shape: {y.shape}\")\n",
    "    feat_shape = X.shape\n",
    "    featNo = feat_shape[-1]\n",
    "    print(featNo)\n",
    "    print(featureNames)\n",
    "    \n",
    "    # split between training and test data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "    print(\"***\")\n",
    "    print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "    print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "    print(\"***\")\n",
    "    x_trainShape = X_train.shape\n",
    "    x_validShape = X_valid.shape\n",
    "\n",
    "    # Assuming featNo and final_activation are defined in your original code\n",
    "    featNo = featNo  # Replace with your actual feature count\n",
    "    final_activation = final_activation  # Replace with your actual activation\n",
    "    \n",
    "    # Create the model\n",
    "    model = resnet_model_implementation(featNo, architecture, final_activation)\n",
    "    \n",
    "    # Your existing custom loss function\n",
    "    from tensorflow.keras.losses import MeanSquaredError\n",
    "    from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "    \n",
    "    custom_loss_fn = make_swe_fsca_loss(\n",
    "        base_loss_fn=MeanSquaredError(),\n",
    "        penalty_weight=0.3,\n",
    "        swe_threshold=0.01,\n",
    "        fsca_threshold=0.01,\n",
    "        mask_value=-1\n",
    "    )\n",
    "    \n",
    "    # Compile with your existing setup\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=custom_loss_fn,\n",
    "        metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # establish the model\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"{inter_model_outWorkspace}/best_weights_{timestamp}.weights.h5\", \n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        mode='min'\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor=\"val_masked_rmse\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    batch_size = 32\n",
    "    train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "    valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[checkpoint, early_stopping]\n",
    "    )\n",
    "\n",
    "    ## plotting results \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "    # 1. Masked MSE (used as loss)\n",
    "    axs[0].plot(history.history['loss'], label='Train MSE Loss')\n",
    "    axs[0].plot(history.history['val_loss'], label='Val MSE Loss')\n",
    "    axs[0].set_ylabel('MSE Loss')\n",
    "    axs[0].set_title('Masked MSE')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    # 2. Masked RMSE\n",
    "    axs[1].plot(history.history['masked_rmse'], label='Train RMSE')\n",
    "    axs[1].plot(history.history['val_masked_rmse'], label='Val RMSE')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].set_title('Masked RMSE')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    # 3. Masked MAE\n",
    "    axs[2].plot(history.history['masked_mae'], label='Train MAE')\n",
    "    axs[2].plot(history.history['val_masked_mae'], label='Val MAE')\n",
    "    axs[2].set_ylabel('MAE')\n",
    "    axs[2].set_title('Masked MAE')\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(inter_model_outWorkspace + \"Model_error_epochs.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add metrics to recurring error tracking sheet\n",
    "    metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "    best_metrics = {}\n",
    "    print(\"\\nValidation Metric Progression:\")\n",
    "    for metric in metrics_to_track:\n",
    "        values = history.history.get(metric, [])\n",
    "        if values:\n",
    "            best_val = min(values)\n",
    "            best_metrics[metric] = best_val\n",
    "            print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "            print(f\"{metric}: Best = {best_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: Not found in history.\")\n",
    "    print(f\"Final activation function: {final_activation}\")\n",
    "    \n",
    "    # Example variables\n",
    "    feature_csv = modelOuptuts + f\"{Domain}_model_featureList_summary.csv\" \n",
    "    column_name = timestamp  \n",
    "    feature_list = featureNames    \n",
    "    new_column_df = pd.DataFrame({column_name: feature_list})\n",
    "    \n",
    "    # If the file already exists, load it and append the new column\n",
    "    if os.path.exists(feature_csv):\n",
    "        existing_df = pd.read_csv(feature_csv)\n",
    "        # Reindex the existing dataframe \n",
    "        max_len = max(len(existing_df), len(new_column_df))\n",
    "        existing_df = existing_df.reindex(range(max_len))\n",
    "        new_column_df = new_column_df.reindex(range(max_len))\n",
    "        # Combine horizontally\n",
    "        combined_df = pd.concat([existing_df, new_column_df], axis=1)\n",
    "    else:\n",
    "        combined_df = new_column_df\n",
    "    \n",
    "    # Save back to CSV\n",
    "    combined_df.to_csv(feature_csv, index=False)\n",
    "    \n",
    "    # add metrics to csv\n",
    "    modelStatsCSV = modelOuptuts + f\"{Domain}_modelSummary_stats.csv\"\n",
    "    \n",
    "    error_stats = {\n",
    "        'ModelRun':[timestamp],\n",
    "        'FeatureNum': [featNo],\n",
    "        'Architecture': [architecture],\n",
    "        'FinalActivation': [final_activation],\n",
    "        'X_TrainShape': [x_trainShape[0]],\n",
    "        'X_ValidShape': [x_validShape[0]],\n",
    "        'RMSE': [best_metrics['val_masked_rmse']],\n",
    "        'MSE': [best_metrics['val_masked_mse']], \n",
    "        'MAE': [best_metrics['val_masked_mae']]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(error_stats)\n",
    "    \n",
    "    # Append or write new file\n",
    "    if os.path.exists(modelStatsCSV):\n",
    "        df.to_csv(modelStatsCSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(modelStatsCSV, index=False)\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid, X, y\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    del history \n",
    "    clear_memory()\n",
    "\n",
    "    # create folder\n",
    "    print(\"\\n ############### MOVING TO TEST GROUPS OF DATA ###############\")\n",
    "    for group_name, year, group_id in test_groups:\n",
    "        print(f\"\\nTesting {group_name}\")\n",
    "        \n",
    "        # Create output folder\n",
    "        out_folder = inter_model_outWorkspace + f\"outTifs_{group_id}_yPreds_tifs/\"\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        \n",
    "        # Load test data\n",
    "        X_train_g, y_train_g, g_train_extents, g_train_crs = target_feature_stacks_testGroups(\n",
    "            year=year,\n",
    "            target_splits_path=WorkspaceBase + f\"test_groups/{group_name}/train/\",\n",
    "            fSCA_path=WorkspaceBase + f\"{year}/fSCA/\",\n",
    "            DMFSCA_path=WorkspaceBase + f\"{year}/DMFSCA/\",\n",
    "            vegetation_path=WorkspaceBase + \"treeCover/\",\n",
    "            landCover_path=land_workspace,\n",
    "            phv_path=WorkspaceBase + \"features/scaled/\",\n",
    "            extension_filter=\".tif\",\n",
    "            desired_shape=(256, 256),\n",
    "            debug_output_folder=\"./debug_outputs/\",\n",
    "            num_of_channels=featNo,\n",
    "            shapeChecks=\"Y\"\n",
    "        )\n",
    "        \n",
    "        X_test_g, y_test_g, g_test_extents, g_test_crs = target_feature_stacks_testGroups(\n",
    "            year=year,\n",
    "            target_splits_path=WorkspaceBase + f\"test_groups/{group_name}/test/\",\n",
    "            fSCA_path=WorkspaceBase + f\"{year}/fSCA/\",\n",
    "            DMFSCA_path=WorkspaceBase + f\"{year}/DMFSCA/\",\n",
    "            vegetation_path=WorkspaceBase + \"treeCover/\",\n",
    "            landCover_path=land_workspace,\n",
    "            phv_path=WorkspaceBase + \"features/scaled/\",\n",
    "            extension_filter=\".tif\",\n",
    "            desired_shape=(256, 256),\n",
    "            debug_output_folder=\"./debug_outputs/\",\n",
    "            num_of_channels=featNo,\n",
    "            shapeChecks=\"Y\"\n",
    "        )\n",
    "        \n",
    "        # Convert to arrays\n",
    "        X_train_array = np.array(X_train_g)\n",
    "        y_train_array = np.array(y_train_g)\n",
    "        X_test_array = np.array(X_test_g)\n",
    "        y_test_array = np.array(y_test_g)\n",
    "        print(f\"X_train shape: {X_train_array.shape}\")\n",
    "        print(f\"y_train shape: {y_train_array.shape}\")\n",
    "        print(f\"X_test shape: {X_test_array.shape}\")\n",
    "        print(f\"y_test shape: {y_test_array.shape}\")\n",
    "        \n",
    "        # add in test model with the weights\n",
    "        test_model = resnet_model_implementation(featNo, architecture, final_activation)\n",
    "\n",
    "        # load weights and test model\n",
    "        best_weights_path = f\"{inter_model_outWorkspace}/best_weights_{timestamp}.weights.h5\"\n",
    "        test_model.load_weights(best_weights_path)\n",
    "        print(\"loaded in best weights\")\n",
    "        \n",
    "        # Compile the model (needed for evaluation metrics)\n",
    "        test_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=custom_loss_fn,\n",
    "            metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "        )\n",
    "\n",
    "        #fine tune model\n",
    "        history_fit = test_model.fit(\n",
    "            X_train_array, y_train_array,\n",
    "            batch_size=16,  # Adjust based on your memory\n",
    "            epochs=10,      # Adjust based on your needs\n",
    "            validation_split=0.2,  # Use 20% of train data for validation\n",
    "            verbose=1\n",
    "        )\n",
    "        # Make predictions\n",
    "        print(\"Making predictions on test data...\")\n",
    "        predictions = test_model.predict(X_test_array, batch_size=32, verbose=1)\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        print(\"Evaluating model on test data...\")\n",
    "        test_metrics = test_model.evaluate(X_test_array, y_test_array, batch_size=32, verbose=1)\n",
    "\n",
    "        # Print test metrics\n",
    "        print(f\"\\nTest Results for {group_name}:\")\n",
    "        print(f\"Masked MSE (Loss): {test_metrics['loss']:.4f}\")\n",
    "        print(f\"Masked RMSE:       {test_metrics['masked_rmse']:.4f}\")\n",
    "        print(f\"Masked MAE:        {test_metrics['masked_mae']:.4f}\")\n",
    "        print(f\"Masked MSE Metric: {test_metrics['masked_mse']:.4f}\")\n",
    "\n",
    "        # save arrays as rasters\n",
    "        # Process predictions and save\n",
    "        for i, pred in enumerate(predictions): # CHECK TO SEE IF THIS IS THE CORRECT SYNTAX\n",
    "            array = pred.reshape((256, 256))\n",
    "            mask = y_test_g[i].reshape((256, 256)) != -1\n",
    "            array_masked = np.where(mask, array, -1)\n",
    "            \n",
    "            save_array_as_raster(\n",
    "                output_path=f\"{out_folder}/prediction_{i}.tif\",\n",
    "                array=array_masked.astype(np.float32),\n",
    "                extent=g_test_extents[i],\n",
    "                crs=g_test_crs[i],\n",
    "                nodata_val=-1\n",
    "            )\n",
    "            \n",
    "        # Clear intermediate arrays immediately\n",
    "        del array, mask, array_masked\n",
    "        del model, custom_loss_fn\n",
    "        del X_train_g, y_train_g, g_train_extents, g_train_crs\n",
    "        del X_test_g, y_test_g, g_test_extents, g_test_crs\n",
    "        del X_test_array, y_test_array, y_pred, metrics\n",
    "        \n",
    "        # Force memory clearing\n",
    "        clear_memory()\n",
    "        print(f\"{group_name} memory cleared.\")\n",
    "    \n",
    "    # End timing for this architecture\n",
    "    end_time = time.time()\n",
    "    total_minutes = (end_time - start_time) / 60\n",
    "    print(f\"\\nArchitecture {architecture} completed in {total_minutes:.2f} minutes\")\n",
    "\n",
    "    try:\n",
    "        f.close()\n",
    "        sys.stdout = sys.__stdout__  # Reset stdout\n",
    "    except:\n",
    "        pass\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c1699-6351-4395-add2-607fcc6e89d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8c1fc-e53f-4827-985c-8eedeca9f75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98233f9c-abc6-4173-b5f2-0917fbbae6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2c700c-1ac3-412d-bb4c-5488b26bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules established\n",
      "Baseline\n",
      "MODEL OUTPUTS TO BE PRINTED TO THIS DOC\n",
      "Processing year 2022\n",
      "\n",
      "Shape of input data\n",
      "feature shape: (426, 256, 256, 22)\n",
      "target shape: (426, 65536)\n",
      "22\n",
      "['fSCA', 'DMFSCA', 'DOY', 'Tree Density', 'LandCover', 'ASOML_CON_waterbodies_1_0_60_albn83', 'ASOML_CON_windScour_60_albn83_scl', 'ASO_CON_aspect_albn83_60m_scl', 'ASO_CON_curv_albn83_60m_scl', 'ASO_CON_DaH_albn83_60m_scl', 'ASO_CON_dem_albn83_60m_scl', 'ASO_CON_GausCurv_albn83_60m_scl', 'ASO_CON_gradMag_60_albn83_scl', 'ASO_CON_lat_albn83_60m_scl', 'ASO_CON_lon_albn83_60m_scl', 'ASO_CON_PlanCurv_albn83_60m_scl', 'ASO_CON_ProCurv_albn83_60m_scl', 'ASO_CON_slope_albn83_60m_scl', 'ASO_CON_stdElv_60_albn83_scl', 'ASO_CON_STDslope_albn83_60m_scl', 'ASO_CON_TPI_albn83_60m_scl', 'ASO_CON_TRASP_albn83_60m_scl']\n",
      "***\n",
      "________________________________ Training and Validation Data Shapes ________________________________\n",
      "Training data shape: (362, 256, 256, 22) (362, 65536)\n",
      "Validation data shape: (64, 256, 256, 22) (64, 65536)\n",
      "***\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 254, 254, 64)      12736     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 254, 254, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_4 (Averag  (None, 127, 127, 64)     0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 125, 125, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 125, 125, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_5 (Averag  (None, 62, 62, 128)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 60, 60, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_6 (Averag  (None, 30, 30, 128)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " average_pooling2d_7 (Averag  (None, 14, 14, 128)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               12845568  \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 65536)             33619968  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,851,136\n",
      "Trainable params: 46,849,216\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_1/batch_normalization_5/FusedBatchNormV3' defined at (most recent call last):\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\etyrr\\AppData\\Local\\Temp\\ipykernel_2432\\20084043.py\", line 184, in <module>\n      history = model.fit(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 850, in call\n      outputs = self._fused_batch_norm(inputs, training=training)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 660, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 634, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'sequential_1/batch_normalization_5/FusedBatchNormV3'\nOOM when allocating tensor with shape[32,64,254,254] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/batch_normalization_5/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_5268]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m DataGenerator(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m    182\u001b[0m valid_generator \u001b[38;5;241m=\u001b[39m DataGenerator(X_valid, y_valid, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m--> 184\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m## plotting results \u001b[39;00m\n\u001b[0;32m    192\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m10\u001b[39m), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/batch_normalization_5/FusedBatchNormV3' defined at (most recent call last):\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\etyrr\\AppData\\Local\\Temp\\ipykernel_2432\\20084043.py\", line 184, in <module>\n      history = model.fit(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 850, in call\n      outputs = self._fused_batch_norm(inputs, training=training)\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 660, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\etyrr\\anaconda3\\envs\\basins-gpu\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py\", line 634, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'sequential_1/batch_normalization_5/FusedBatchNormV3'\nOOM when allocating tensor with shape[32,64,254,254] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/batch_normalization_5/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_5268]"
     ]
    }
   ],
   "source": [
    "# pseduo code\n",
    "import sys\n",
    "sys.path.append(\"D:/ASOML/SNOCONE\")\n",
    "from CNN_memoryOptimization import clear_memory, memory_efficient_prediction, DataGenerator\n",
    "from CNN_preProcessing import min_max_scale, read_aligned_raster, save_array_as_raster, target_feature_stacks, target_feature_stacks_testGroups\n",
    "from CNN_benchmarks import swe_fsca_consistency_loss_fn, make_swe_fsca_loss, masked_loss_fn, masked_mse, masked_mae, masked_rmse\n",
    "from CNN_modelArchitectureBlocks import conv_block, identity_block, basic_block, resnet_model_implementation, model_predict, load_and_prepare_model, Baseline_CNN\n",
    "import rasterio\n",
    "import shap\n",
    "import pandas as pd\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "import psutil\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.losses import Loss\n",
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "# years = list(range(2022, 2023))\n",
    "start_year = 2022\n",
    "end_year = 2022\n",
    "Domain = \"Rockies\"\n",
    "GPU = \"N\"\n",
    "\n",
    "# workspaces\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "DMFSCAWorkspace = WorkspaceBase + \"Rockies_DMFSCA/\"\n",
    "final_activation = 'relu'\n",
    "architectures = ['Baseline']  # Options are: Baseline, ResNet18, ResNet34, ResNet50, CustomSWE\n",
    "\n",
    "test_groups = [\n",
    "            ('Group1', 2025, 'G1'),\n",
    "            ('Group2', 2025, 'G2'), \n",
    "            ('Group3', 2025, 'G3'),\n",
    "            ('Group4', 2025, 'G4'),\n",
    "            ('Group5', 2025, 'G5'),\n",
    "            ('Group6', 2025, 'G6')\n",
    "        ]\n",
    "\n",
    "_mae_metric = MeanAbsoluteError()\n",
    "_mse_metric = MeanSquaredError()\n",
    "_rmse_metric = MeanSquaredError()\n",
    "\n",
    "# Check GPU availability\n",
    "if GPU == \"Y\":\n",
    "    print(\"\\n Checking and start running on GPU\")\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Configure GPU memory growth (prevents TensorFlow from allocating all GPU memory)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Verify TensorFlow is using GPU\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"GPU computation result:\", c)\n",
    "\n",
    "for architecture in architectures:\n",
    "    print(architecture)\n",
    "    # shapeChecks = \"N\"\n",
    "    ## seting folder\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # create folder for model outputs\n",
    "    os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "    inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n",
    "    \n",
    "    # f = open(inter_model_outWorkspace + f\"code_output_{timestamp}.txt\", \"a\")\n",
    "    # sys.stdout = f\n",
    "    print(\"MODEL OUTPUTS TO BE PRINTED TO THIS DOC\")\n",
    "    X, y, featureNames = target_feature_stacks(start_year=start_year, \n",
    "                                               end_year=end_year, \n",
    "                                               WorkspaceBase=WorkspaceBase, \n",
    "                                               ext = \"nonull_fnl.tif\", \n",
    "                                               vegetation_path = tree_workspace, \n",
    "                                               landCover_path = land_workspace, \n",
    "                                               phv_path = phv_features , \n",
    "                                               target_shape=(256,256),\n",
    "                                               shapeChecks=\"Y\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Shape of input data\")\n",
    "    print(f\"feature shape: {X.shape}\")\n",
    "    print(f\"target shape: {y.shape}\")\n",
    "    feat_shape = X.shape\n",
    "    featNo = feat_shape[-1]\n",
    "    print(featNo)\n",
    "    print(featureNames)\n",
    "    \n",
    "    # split between training and test data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "    print(\"***\")\n",
    "    print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "    print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "    print(\"***\")\n",
    "    x_trainShape = X_train.shape\n",
    "    x_validShape = X_valid.shape\n",
    "\n",
    "    # _mae_metric = MeanAbsoluteError()\n",
    "    # _mse_metric = MeanSquaredError()\n",
    "    # _rmse_metric = MeanSquaredError()\n",
    "    \n",
    "    # Assuming featNo and final_activation are defined in your original code\n",
    "    featNo = featNo  # Replace with your actual feature count\n",
    "    final_activation = final_activation  # Replace with your actual activation\n",
    "    \n",
    "    # Create the model\n",
    "    model = resnet_model_implementation(featNo, architecture, final_activation)\n",
    "    \n",
    "    # Your existing custom loss function\n",
    "    from tensorflow.keras.losses import MeanSquaredError\n",
    "    from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "    \n",
    "    custom_loss_fn = make_swe_fsca_loss(\n",
    "        base_loss_fn=MeanSquaredError(),\n",
    "        penalty_weight=0.3,\n",
    "        swe_threshold=0.01,\n",
    "        fsca_threshold=0.01,\n",
    "        mask_value=-1\n",
    "    )\n",
    "    \n",
    "    # Compile with your existing setup\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=custom_loss_fn,\n",
    "        metrics=[masked_rmse, masked_mae, masked_mse]\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # establish the model\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", monitor=\"val_loss\",\n",
    "        verbose=1, save_best_only=True, mode='min'\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor=\"val_masked_rmse\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    batch_size = 32\n",
    "    train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "    valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=valid_generator,\n",
    "        epochs=100,\n",
    "        callbacks=[checkpoint, early_stopping]\n",
    "    )\n",
    "\n",
    "    ## plotting results \n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "    # 1. Masked MSE (used as loss)\n",
    "    axs[0].plot(history.history['loss'], label='Train MSE Loss')\n",
    "    axs[0].plot(history.history['val_loss'], label='Val MSE Loss')\n",
    "    axs[0].set_ylabel('MSE Loss')\n",
    "    axs[0].set_title('Masked MSE')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    # 2. Masked RMSE\n",
    "    axs[1].plot(history.history['masked_rmse'], label='Train RMSE')\n",
    "    axs[1].plot(history.history['val_masked_rmse'], label='Val RMSE')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].set_title('Masked RMSE')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    # 3. Masked MAE\n",
    "    axs[2].plot(history.history['masked_mae'], label='Train MAE')\n",
    "    axs[2].plot(history.history['val_masked_mae'], label='Val MAE')\n",
    "    axs[2].set_ylabel('MAE')\n",
    "    axs[2].set_title('Masked MAE')\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(inter_model_outWorkspace + \"Model_error_epochs.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add metrics to recurring error tracking sheet\n",
    "    metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "    best_metrics = {}\n",
    "    print(\"\\nValidation Metric Progression:\")\n",
    "    for metric in metrics_to_track:\n",
    "        values = history.history.get(metric, [])\n",
    "        if values:\n",
    "            best_val = min(values)\n",
    "            best_metrics[metric] = best_val\n",
    "            print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "            print(f\"{metric}: Best = {best_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: Not found in history.\")\n",
    "    print(f\"Final activation function: {final_activation}\")\n",
    "    \n",
    "    # Example variables\n",
    "    feature_csv = modelOuptuts + f\"{Domain}_model_featureList_summary.csv\" \n",
    "    column_name = timestamp  \n",
    "    feature_list = featureNames    \n",
    "    new_column_df = pd.DataFrame({column_name: feature_list})\n",
    "    \n",
    "    # If the file already exists, load it and append the new column\n",
    "    if os.path.exists(feature_csv):\n",
    "        existing_df = pd.read_csv(feature_csv)\n",
    "        # Reindex the existing dataframe \n",
    "        max_len = max(len(existing_df), len(new_column_df))\n",
    "        existing_df = existing_df.reindex(range(max_len))\n",
    "        new_column_df = new_column_df.reindex(range(max_len))\n",
    "        # Combine horizontally\n",
    "        combined_df = pd.concat([existing_df, new_column_df], axis=1)\n",
    "    else:\n",
    "        combined_df = new_column_df\n",
    "    \n",
    "    # Save back to CSV\n",
    "    combined_df.to_csv(feature_csv, index=False)\n",
    "    \n",
    "    # add metrics to csv\n",
    "    modelStatsCSV = modelOuptuts + f\"{Domain}_modelSummary_stats.csv\"\n",
    "    \n",
    "    error_stats = {\n",
    "        'ModelRun':[timestamp],\n",
    "        'FeatureNum': [featNo],\n",
    "        'Architecture': [architecture],\n",
    "        'FinalActivation': [final_activation],\n",
    "        'X_TrainShape': [x_trainShape[0]],\n",
    "        'X_ValidShape': [x_validShape[0]],\n",
    "        'RMSE': [best_metrics['val_masked_rmse']],\n",
    "        'MSE': [best_metrics['val_masked_mse']], \n",
    "        'MAE': [best_metrics['val_masked_mae']]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(error_stats)\n",
    "    \n",
    "    # Append or write new file\n",
    "    if os.path.exists(modelStatsCSV):\n",
    "        df.to_csv(modelStatsCSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(modelStatsCSV, index=False)\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid, X, y\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    del history \n",
    "    clear_memory()\n",
    "\n",
    "    # create folder\n",
    "    print(\"\\n ############### MOVING TO TEST GROUPS OF DATA ###############\")\n",
    "    for group_name, year, group_id in test_groups:\n",
    "        print(f\"\\nTesting {group_name}\")\n",
    "        \n",
    "        # Create output folder\n",
    "        out_folder = inter_model_outWorkspace + f\"outTifs_{group_id}_yPreds_tifs/\"\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        \n",
    "        # Load test data\n",
    "        X_train_g, y_train_g, g_train_extents, g_train_crs = target_feature_stacks_testGroups(\n",
    "            year=year,\n",
    "            target_splits_path=WorkspaceBase + f\"test_groups/{group_name}/train/\",\n",
    "            fSCA_path=WorkspaceBase + f\"{year}/fSCA/\",\n",
    "            DMFSCA_path=WorkspaceBase + f\"{year}/DMFSCA/\",\n",
    "            vegetation_path=WorkspaceBase + \"treeCover/\",\n",
    "            landCover_path=land_workspace,\n",
    "            phv_path=WorkspaceBase + \"features/scaled/\",\n",
    "            extension_filter=\".tif\",\n",
    "            desired_shape=(256, 256),\n",
    "            debug_output_folder=\"./debug_outputs/\",\n",
    "            num_of_channels=featNo,\n",
    "            shapeChecks=\"Y\"\n",
    "        )\n",
    "        \n",
    "        X_test_g, y_test_g, g_test_extents, g_test_crs = target_feature_stacks_testGroups(\n",
    "            year=year,\n",
    "            target_splits_path=WorkspaceBase + f\"test_groups/{group_name}/test/\",\n",
    "            fSCA_path=WorkspaceBase + f\"{year}/fSCA/\",\n",
    "            DMFSCA_path=WorkspaceBase + f\"{year}/DMFSCA/\",\n",
    "            vegetation_path=WorkspaceBase + \"treeCover/\",\n",
    "            landCover_path=land_workspace,\n",
    "            phv_path=WorkspaceBase + \"features/scaled/\",\n",
    "            extension_filter=\".tif\",\n",
    "            desired_shape=(256, 256),\n",
    "            debug_output_folder=\"./debug_outputs/\",\n",
    "            num_of_channels=featNo,\n",
    "            shapeChecks=\"Y\"\n",
    "        )\n",
    "        \n",
    "        # Convert to arrays\n",
    "        X_test_array = np.array(X_test_g)\n",
    "        y_test_array = np.array(y_test_g)\n",
    "        \n",
    "        print(f\"X_test shape: {X_test_array.shape}\")\n",
    "        print(f\"y_test shape: {y_test_array.shape}\")\n",
    "        \n",
    "        # Load model (recreate custom loss each time to avoid memory leaks)\n",
    "        custom_loss_fn = make_swe_fsca_loss(\n",
    "            base_loss_fn=MeanSquaredError(),\n",
    "            penalty_weight=0.3,\n",
    "            swe_threshold=0.01,\n",
    "            fsca_threshold=0.01,\n",
    "            mask_value=-1\n",
    "        )\n",
    "        \n",
    "        model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", \n",
    "                         custom_objects={\n",
    "                             'loss': custom_loss_fn,\n",
    "                             'swe_fsca_consistency_loss_fn': swe_fsca_consistency_loss_fn,\n",
    "                             'masked_rmse': masked_rmse,\n",
    "                             'masked_mae': masked_mae,\n",
    "                             'masked_mse': masked_mse\n",
    "                         })\n",
    "        \n",
    "        # Use memory-efficient prediction\n",
    "        y_pred = memory_efficient_prediction(model, X_test_array, batch_size=3)  # Reduced batch size\n",
    "        \n",
    "        # Process predictions and save\n",
    "        for i, pred in enumerate(y_pred):\n",
    "            array = pred.reshape((256, 256))\n",
    "            mask = y_test_g[i].reshape((256, 256)) != -1\n",
    "            array_masked = np.where(mask, array, -1)\n",
    "            \n",
    "            save_array_as_raster(\n",
    "                output_path=f\"{out_folder}/prediction_{i}.tif\",\n",
    "                array=array_masked.astype(np.float32),\n",
    "                extent=g_test_extents[i],\n",
    "                crs=g_test_crs[i],\n",
    "                nodata_val=-1\n",
    "            )\n",
    "            \n",
    "            # Clear intermediate arrays immediately\n",
    "            del array, mask, array_masked\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = model.evaluate(X_test_array, y_test_array, batch_size=16, return_dict=True)  # Smaller batch\n",
    "        print(f\"\\n{group_name} Metrics:\")\n",
    "        print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "        print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "        print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "        print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")\n",
    "        \n",
    "        # Comprehensive cleanup for this group\n",
    "        del model, custom_loss_fn\n",
    "        del X_train_g, y_train_g, g_train_extents, g_train_crs\n",
    "        del X_test_g, y_test_g, g_test_extents, g_test_crs\n",
    "        del X_test_array, y_test_array, y_pred, metrics\n",
    "        \n",
    "        # Force memory clearing\n",
    "        clear_memory()\n",
    "        print(f\"{group_name} memory cleared.\")\n",
    "    \n",
    "    # End timing for this architecture\n",
    "    end_time = time.time()\n",
    "    total_minutes = (end_time - start_time) / 60\n",
    "    print(f\"\\nArchitecture {architecture} completed in {total_minutes:.2f} minutes\")\n",
    "\n",
    "    try:\n",
    "        f.close()\n",
    "        sys.stdout = sys.__stdout__  # Reset stdout\n",
    "    except:\n",
    "        pass\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f426cae-b395-4348-9c53-06aadf8e6145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: D:\\ASOML\\Rockies\\modelOutputs\\fromAlpine\\20250708_114938\\best_model_20250708_114938.keras\n",
      "File exists: True\n",
      "File size: 562278163 bytes\n",
      "File is corrupted: Unable to open file (file signature not found)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "model_path = r\"D:\\ASOML\\Rockies\\modelOutputs\\fromAlpine\\20250708_114938\\best_model_20250708_114938.keras\"\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"File exists: {os.path.exists(model_path)}\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"File size: {os.path.getsize(model_path)} bytes\")\n",
    "    \n",
    "    # Try to open as HDF5 to check if it's valid\n",
    "    try:\n",
    "        with h5py.File(model_path, 'r') as f:\n",
    "            print(\"File is a valid HDF5 file\")\n",
    "            print(f\"Keys: {list(f.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"File is corrupted: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c1699-6351-4395-add2-607fcc6e89d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
